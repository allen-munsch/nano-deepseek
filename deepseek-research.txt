Model Arena-Hard AlpacaEval 2.0
DeepSeek-V2.5-0905 76.2 50.5
Qwen2.5-72B-Instruct 81.2 49.1
LLaMA-3.1 405B 69.3 40.5
GPT-4o-0513 80.4 51.1
Claude-Sonnet-3.5-1022 85.2 52.0
DeepSeek-V3 85.5 70.0
Table 7 | English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-
controlled win rate as the metric.
Chinese Benchmarks. Qwen and DeepSeek are two representative model series with robust
support for both Chinese and English. On the factual benchmark Chinese SimpleQA, DeepSeek-
V3 surpasses Qwen2.5-72B by 16.4 points, despite Qwen2.5 being trained on a larger corpus
compromising 18T tokens, which are 20% more than the 14.8T tokens that DeepSeek-V3 is
pre-trained on.
On C-Eval, a representative benchmark for Chinese educational knowledge evaluation, and
CLUEWSC (Chinese Winograd Schema Challenge), DeepSeek-V3 and Qwen2.5-72B exhibit
similar performance levels, indicating that both models are well-optimized for challenging
Chinese-language reasoning and educational tasks.
5.3.3. Open-Ended Evaluation
In addition to standard benchmarks, we also evaluate our models on open-ended generation
tasks using LLMs as judges, with the results shown in Table 7. Specifically, we adhere to
the original configurations of AlpacaEval 2.0 (Dubois et al., 2024) and Arena-Hard (Li et al.,
2024a), which leverage GPT-4-Turbo-1106 as judges for pairwise comparisons. On Arena-Hard,
DeepSeek-V3 achieves an impressive win rate of over 86% against the baseline GPT-4-0314,
performing on par with top-tier models like Claude-Sonnet-3.5-1022. This underscores the
robust capabilities of DeepSeek-V3, especially in dealing with complex prompts, including
coding and debugging tasks. Furthermore, DeepSeek-V3 achieves a groundbreaking milestone
as the first open-source model to surpass 85% on the Arena-Hard benchmark. This achievement
significantly bridges the performance gap between open-source and closed-source models,
setting a new standard for what open-source models can accomplish in challenging domains.
Similarly, DeepSeek-V3 showcases exceptional performance on AlpacaEval 2.0, outperform-
ing both closed-source and open-source models. This demonstrates its outstanding proficiency in
writing tasks and handling straightforward question-answering scenarios. Notably, it surpasses
DeepSeek-V2.5-0905 by a significant margin of 20%, highlighting substantial improvements in
tackling simple tasks and showcasing the effectiveness of its advancements.
5.3.4. DeepSeek-V3 as a Generative Reward Model
We compare the judgment ability of DeepSeek-V3 with state-of-the-art models, namely GPT-4o
and Claude-3.5. Table 8 presents the performance of these models in RewardBench (Lambert
et al., 2024). DeepSeek-V3 achieves performance on par with the best versions of GPT-4o-0806
and Claude-3.5-Sonnet-1022, while surpassing other versions. Additionally, the judgment ability
of DeepSeek-V3 can also be enhanced by the voting technique. Therefore, we employ DeepSeek-
V3 along with voting to offer self-feedback on open-ended questions, thereby improving the
33
Model Chat Chat-Hard Safety Reasoning Average
GPT-4o-0513 96.6 70.4 86.7 84.9 84.7
GPT-4o-0806 96.1 76.1 88.1 86.6 86.7
GPT-4o-1120 95.8 71.3 86.2 85.2 84.6
Claude-3.5-sonnet-0620 96.4 74.0 81.6 84.7 84.2
Claude-3.5-sonnet-1022 96.4 79.7 91.1 87.6 88.7
DeepSeek-V3 96.9 79.8 87.0 84.3 87.0
DeepSeek-V3 (maj@6) 96.9 82.6 89.5 89.2 89.6
Table 8 | Performances of GPT-4o, Claude-3.5-sonnet and DeepSeek-V3 on RewardBench.
Model LiveCodeBench-CoT MATH-500
Pass@1 Length Pass@1 Length
DeepSeek-V2.5 Baseline 31.1 718 74.6 769
DeepSeek-V2.5 +R1 Distill 37.4 783 83.2 1510
Table 9 | The contribution of distillation from DeepSeek-R1. The evaluation settings of Live-
CodeBench and MATH-500 are the same as in Table 6.
effectiveness and robustness of the alignment process.
5.4. Discussion
5.4.1. Distillation from DeepSeek-R1
We ablate the contribution of distillation from DeepSeek-R1 based on DeepSeek-V2.5. The
baseline is trained on short CoT data, whereas its competitor uses data generated by the expert
checkpoints described above.
Table 9 demonstrates the effectiveness of the distillation data, showing significant improve-
ments in both LiveCodeBench and MATH-500 benchmarks. Our experiments reveal an inter-
esting trade-off: the distillation leads to better performance but also substantially increases the
average response length. To maintain a balance between model accuracy and computational
efficiency, we carefully selected optimal settings for DeepSeek-V3 in distillation.
Our research suggests that knowledge distillation from reasoning models presents a promis-
ing direction for post-training optimization. While our current work focuses on distilling data
from mathematics and coding domains, this approach shows potential for broader applications
across various task domains. The effectiveness demonstrated in these specific areas indicates
that long-CoT distillation could be valuable for enhancing model performance in other cogni-
tive tasks requiring complex reasoning. Further exploration of this approach across different
domains remains an important direction for future research.
5.4.2. Self-Rewarding
Rewards play a pivotal role in RL, steering the optimization process. In domains where verifica-
tion through external tools is straightforward, such as some coding or mathematics scenarios, RL
demonstrates exceptional efficacy. However, in more general scenarios, constructing a feedback
34
mechanism through hard coding is impractical. During the development of DeepSeek-V3, for
these broader contexts, we employ the constitutional AI approach (Bai et al., 2022), leveraging
the voting evaluation results of DeepSeek-V3 itself as a feedback source. This method has
produced notable alignment effects, significantly enhancing the performance of DeepSeek-V3
in subjective evaluations. By integrating additional constitutional inputs, DeepSeek-V3 can
optimize towards the constitutional direction. We believe that this paradigm, which combines
supplementary information with LLMs as a feedback source, is of paramount importance. The
LLM serves as a versatile processor capable of transforming unstructured information from
diverse scenarios into rewards, ultimately facilitating the self-improvement of LLMs. Beyond
self-rewarding, we are also dedicated to uncovering other general and scalable rewarding
methods to consistently advance the model capabilities in general scenarios.
5.4.3. Multi-Token Prediction Evaluation
Instead of predicting just the next single token, DeepSeek-V3 predicts the next 2 tokens through
the MTP technique. Combined with the framework of speculative decoding (Leviathan et al.,
2023; Xia et al., 2023), it can significantly accelerate the decoding speed of the model. A natural
question arises concerning the acceptance rate of the additionally predicted token. Based on
our evaluation, the acceptance rate of the second token prediction ranges between 85% and 90%
across various generation topics, demonstrating consistent reliability. This high acceptance rate
enables DeepSeek-V3 to achieve a significantly improved decoding speed, delivering 1.8 times
TPS (Tokens Per Second).
6. Conclusion, Limitations, and Future Directions
In this paper, we introduce DeepSeek-V3, a large MoE language model with 671B total pa-
rameters and 37B activated parameters, trained on 14.8T tokens. In addition to the MLA and
DeepSeekMoE architectures, it also pioneers an auxiliary-loss-free strategy for load balancing
and sets a multi-token prediction training objective for stronger performance. The training of
DeepSeek-V3 is cost-effective due to the support of FP8 training and meticulous engineering op-
timizations. The post-training also makes a success in distilling the reasoning capability from the
DeepSeek-R1 series of models. Comprehensive evaluations demonstrate that DeepSeek-V3 has
emerged as the strongest open-source model currently available, and achieves performance com-
parable to leading closed-source models like GPT-4o and Claude-3.5-Sonnet. Despite its strong
performance, it also maintains economical training costs. It requires only 2.788M H800 GPU
hours for its full training, including pre-training, context length extension, and post-training.
While acknowledging its strong performance and cost-effectiveness, we also recognize that
DeepSeek-V3 has some limitations, especially on the deployment. Firstly, to ensure efficient
inference, the recommended deployment unit for DeepSeek-V3 is relatively large, which might
pose a burden for small-sized teams. Secondly, although our deployment strategy for DeepSeek-
V3 has achieved an end-to-end generation speed of more than two times that of DeepSeek-V2,
there still remains potential for further enhancement. Fortunately, these limitations are expected
to be naturally addressed with the development of more advanced hardware.
DeepSeek consistently adheres to the route of open-source models with longtermism, aiming
to steadily approach the ultimate goal of AGI (Artificial General Intelligence). In the future, we
plan to strategically invest in research across the following directions.
• We will consistently study and refine our model architectures, aiming to further improve
35
both the training and inference efficiency, striving to approach efficient support for infinite
context length. Additionally, we will try to break through the architectural limitations of
Transformer, thereby pushing the boundaries of its modeling capabilities.
• We will continuously iterate on the quantity and quality of our training data, and explore
the incorporation of additional training signal sources, aiming to drive data scaling across
a more comprehensive range of dimensions.
• We will consistently explore and iterate on the deep thinking capabilities of our models,
aiming to enhance their intelligence and problem-solving abilities by expanding their
reasoning length and depth.
• We will explore more comprehensive and multi-dimensional model evaluation methods to
prevent the tendency towards optimizing a fixed set of benchmarks during research, which
may create a misleading impression of the model capabilities and affect our foundational
assessment
